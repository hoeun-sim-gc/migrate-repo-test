{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input job parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_guid': '80a1ca07-41f9-4f34-b7bb-d01b477bc78d',\n",
       " 'job_name': 'PAT_Test_AIG',\n",
       " 'type_of_rating': 'PSOLD',\n",
       " 'coverage': 'Building + Contents + Time Element',\n",
       " 'peril_subline': 'All_Perils',\n",
       " 'subject_premium': 100000000.0,\n",
       " 'loss_alae_ratio': 1,\n",
       " 'average_accident_date': '1/1/2022',\n",
       " 'trend_factor': 1.035,\n",
       " 'additional_coverage': 2,\n",
       " 'deductible_treatment': 'Retains Limit',\n",
       " 'server': 'DFWCAT-RMS5SQL1',\n",
       " 'edm_database': 'RMS_EPL_AIG_PAT_Testing_202107_EDM181',\n",
       " 'rdm_database': 'RMS_EPL_AIG_PAT_Testing_202107_RDM181',\n",
       " 'portinfoid': 6,\n",
       " 'perilid': 5,\n",
       " 'analysisid': 5,\n",
       " 'user_name': 'cxiao',\n",
       " 'user_ema': 'chengyou.xiao@guycarp.com'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \r\n",
    "\r\n",
    "job_para = json.load(open('data/pat_job_test.json'))\r\n",
    "job_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from pat import PatAnalysis, PatFlag\r\n",
    "\r\n",
    "job = PatAnalysis(job_para)\r\n",
    "job.conn_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.extract_edm_rdm()\r\n",
    "job.check_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net of Fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting PsuedoPolicies by FAC Layering\r\n",
    "if len(job.df_fac) > 0:\r\n",
    "    job.net_of_fac()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate with PSOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.allocate_with_psold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = job.df_pat.reset_index(drop=True)\r\n",
    "\r\n",
    "df2 =pd.read_csv(r'C:\\_Working\\PAT_20201019\\__temp\\dfpat.csv',dtype={\r\n",
    "                     'LocationIDStack': str\r\n",
    "                 }, index_col=0).reset_index(drop=True)\r\n",
    "df2.PseudoPolicyID = df2.PseudoPolicyID.str.replace(' _ ', '_').reset_index(drop=True) \r\n",
    "df1.columns = df2.columns\r\n",
    "\r\n",
    "\r\n",
    "# df1 = df1.sort_values(by=['PseudoPolicyID','LocationIDStack','PseudoLayerID']).reset_index(drop=True)\r\n",
    "# df2 = df2.sort_values(by=['PseudoPolicyID','LocationIDStack','PseudoLayerID']).reset_index(drop=True)\r\n",
    "# df1 = df1.sort_values(by=['PseudoPolicyID']).reset_index(drop=True)\r\n",
    "# df2 = df2.sort_values(by=['PseudoPolicyID']).reset_index(drop=True)\r\n",
    "# df2 = df2.sort_values(by=['PseudoPolicyID','FacAttachment', 'FacLimit','FacCeded']).reset_index(drop=True)\r\n",
    "\r\n",
    "# df1 = df1.sort_values(by=['PseudoPolicyID','Retention']).reset_index(drop=True)\r\n",
    "# df2 = df2.sort_values(by=['PseudoPolicyID','Retention']).reset_index(drop=True)\r\n",
    "\r\n",
    "\r\n",
    "print(df1.shape, df2.shape)\r\n",
    "pd.DataFrame(df1.dtypes,columns=['DF1']).join(pd.DataFrame(df2.dtypes,columns=['DF2']),how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_string_dtype\r\n",
    "from pandas.api.types import is_numeric_dtype\r\n",
    "\r\n",
    "cmp =pd.DataFrame(columns=['name','type', 'match','df1_nan','df2_nan'])\r\n",
    "for c in df1.columns:\r\n",
    "    if is_string_dtype(df1[c]):\r\n",
    "        if np.all(df1[c].fillna(value=\"\") != df2[c].fillna(value=\"\")):\r\n",
    "            cmp = cmp.append({'name': c,'type':df1[c].dtype,'match':'NO'}, ignore_index=True)\r\n",
    "        else:\r\n",
    "            cmp = cmp.append({'name': c,'type':df1[c].dtype,'match':'YES'}, ignore_index=True)\r\n",
    "    elif is_numeric_dtype(df1[c]):\r\n",
    "        dif =np.max(np.abs(df1[c].astype('float') - df2[c].astype('float')) / np.maximum(df1[c].astype('float'), df2[c].astype('float')) )\r\n",
    "        cmp = cmp.append({'name': c,'type':df1[c].dtype,'match':dif,\r\n",
    "            'df1_nan':np.sum(np.isnan(df1[c])),\r\n",
    "            'df2_nan':np.sum(np.isnan(df2[c]))\r\n",
    "        }, ignore_index=True)\r\n",
    "    else:\r\n",
    "        cmp = cmp.append({'name': c,'type':df1[c].dtype,'match':'UNK'}, ignore_index=True)\r\n",
    "\r\n",
    "cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.get_validation_counts()\r\n",
    "\r\n",
    "# df_a, df_b, df_c =job.get_validation_data()\r\n",
    "# print(df_a.shape, df_b.shape, df_c.shape)\r\n",
    "\r\n",
    "# df_b.sort_values(['LocationIDStack', 'AOI']).to_csv(r'C:\\_Working\\PAT_20201019\\__temp\\loc_correction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, b'Analysis submitted!')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\r\n",
    "import json\r\n",
    "\r\n",
    "files = {'para': ('pat_para', open('data/pat_job_test.json'))}\r\n",
    "ret = requests.post(\"http://localhost:5000/api/Jobs\", files = files)\r\n",
    "ret.status_code, ret.content"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "127074f8ccc7e6be4e4813583b3c2191c4b36279ad1345c0e84a925cf005a6c8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
